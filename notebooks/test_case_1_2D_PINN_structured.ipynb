{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6751a26-e630-41fc-8a3f-a1fba14e6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from typing import List, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "def domain(n,pars):\n",
    "\n",
    "    t = np.random.uniform(low=0,high=pars['tf'],size=(n,1))\n",
    "    x = np.random.uniform(low=pars['xi'],high=pars['xf'],size=(n,1))\n",
    "    y = np.random.uniform(low=pars['yi'],high=pars['yf'],size=(n,1))\n",
    "\n",
    "    X = np.hstack((t,x,y))\n",
    "\n",
    "    return X\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    # Define the MLP\n",
    "\n",
    "    def __init__(\n",
    "        self, pars, device\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Add number of MLP input and outputs to the layers list\n",
    "        layers = [3,*pars['layers'],2]\n",
    "        \n",
    "        # Built the MLP\n",
    "        modules = []\n",
    "        for _in, _out in list(zip(layers, layers[1:])):\n",
    "            modules.append(nn.Linear(_in, _out))\n",
    "            modules.append(ResidualBlock(_out))\n",
    "        \n",
    "        # Remove last block\n",
    "        modules.pop()\n",
    "\n",
    "        self.model = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # Forward pass\n",
    "        Y_n = self.model(X)\n",
    "        Y_p = self.particular_solution(X)\n",
    "        D = self.boundary_distance(X)\n",
    "\n",
    "        return D * Y_n + (1-D) * Y_p\n",
    "\n",
    "    def particular_solution(self,X):\n",
    "        x = X[:,1].reshape(-1, 1)\n",
    "        y = X[:,2].reshape(-1, 1)\n",
    "\n",
    "        u = torch.sin(np.pi*((1+x+y)/2))\n",
    "\n",
    "        return u\n",
    "\n",
    "    def boundary_distance(self,X):\n",
    "\n",
    "        alpha = 26.4 # Reaches 0.99 at t = 0.1\n",
    "        #alpha = 10.56 # Reaches 0.99 at t = 0.25\n",
    "\n",
    "        t = X[:,0].reshape(-1, 1)\n",
    "        x = X[:,1].reshape(-1, 1)\n",
    "        y = X[:,2].reshape(-1, 1)\n",
    "\n",
    "        dt = torch.tanh(t*alpha)\n",
    "        dx = 4*(0.5*x+0.5)*(1-(0.5*x+0.5))\n",
    "        dy = 4*(0.5*y+0.5)*(1-(0.5*y+0.5))\n",
    "\n",
    "        return torch.hstack((dt*dx*dy,dt*dx*dy))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    # Define a block with two layers and a residual connection\n",
    "    def __init__(self,_size:int):\n",
    "        super().__init__()\n",
    "        self.Layer1 = nn.Tanh()\n",
    "        self.Linear = nn.Linear(_size, _size)\n",
    "        self.Layer2 = nn.Tanh()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x + self.Layer2(self.Linear(self.Layer1(x)))\n",
    "\n",
    "class PINN:\n",
    "    def __init__(self, nf, pars: dict, device: torch.device = 'cuda') -> None:\n",
    "\n",
    "        # Parameters\n",
    "        self.pars = pars\n",
    "        self.device = device\n",
    "        self.nf = nf\n",
    "        self.nu = pars['nu']\n",
    "\n",
    "        self.ls_f1 = torch.tensor(0).to(self.device)\n",
    "        self.ls_f2 = torch.tensor(0).to(self.device)\n",
    "        self.ls_f = torch.tensor(0).to(self.device)\n",
    "        self.ls_s = torch.tensor(0).to(self.device)\n",
    "\n",
    "        # Sample points\n",
    "        self.sample_points()\n",
    "        self.zeros = torch.zeros(self.X_f.shape).to(self.device)\n",
    "\n",
    "        # Initialize Network\n",
    "        self.net = MLP(pars,device)\n",
    "        self.net = self.net.to(device)\n",
    "\n",
    "        if pars['loss_type'] == 'l1':\n",
    "            self.loss = nn.L1Loss().to(device)\n",
    "        elif pars['loss_type'] == 'mse':\n",
    "            self.loss = nn.MSELoss().to(device)\n",
    "\n",
    "        self.min_ls_tol = 0.01\n",
    "        self.min_ls_wait = 10000\n",
    "        self.min_ls_window = 1000\n",
    "\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        self.ls = 0\n",
    "        self.iter = 0\n",
    "\n",
    "        self.ls_hist = np.zeros((pars['epochs'],2))\n",
    "\n",
    "        # Optimizer parameters\n",
    "        if pars['opt_method'] == 'adam':\n",
    "            self.optimizer = torch.optim.Adam(self.net.parameters(),lr=pars['opt_lr'])\n",
    "        elif pars['opt_method'] == 'lbfgs':\n",
    "            self.optimizer = torch.optim.LBFGS(self.net.parameters(),lr=pars['opt_lr'])\n",
    "        elif pars['opt_method'] == 'sgd':\n",
    "            self.optimizer = torch.optim.SGD(self.net.parameters(),lr=pars['opt_lr'])\n",
    "        else:\n",
    "            raise Exception(\"Unknown optimization method\")\n",
    "\n",
    "    def sample_points(self):\n",
    "        X_f = domain(self.nf,self.pars)\n",
    "        self.X_f = torch.tensor(X_f,dtype=torch.float,requires_grad=True).to(self.device)\n",
    "\n",
    "    def eq_loss(self, X: torch.Tensor):\n",
    "\n",
    "        # Forward pass\n",
    "        t = X[:,0].reshape(-1, 1)\n",
    "        x = X[:,1].reshape(-1, 1)\n",
    "        y = X[:,2].reshape(-1, 1)\n",
    "        Y = self.net(torch.hstack((t,x,y)))\n",
    "        \n",
    "        u = Y[:,0].reshape(-1, 1)\n",
    "\n",
    "        # Get derivatives\n",
    "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u),\n",
    "                                  retain_graph=True, create_graph=True)[0]\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u),\n",
    "                                  retain_graph=True, create_graph=True)[0]\n",
    "        u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u),\n",
    "                                  retain_graph=True, create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u),\n",
    "                                  retain_graph=True, create_graph=True)[0]\n",
    "        u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u),\n",
    "                                  retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        # Compute residuals\n",
    "        R1 = u_t + 2*u*u_x + 2*u*u_y - 0.2*(u_xx + u_yy)\n",
    "\n",
    "        self.ls_f = self.loss(R1,torch.zeros_like(R1))\n",
    "\n",
    "        return self.ls_f\n",
    "\n",
    "    def closure(self) -> torch.nn:\n",
    "        if self.nf > 0:\n",
    "            self.ls_f = self.eq_loss(self.X_f)\n",
    "            \n",
    "        if self.nf > 0:\n",
    "            self.ls = self.ls_f\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.ls.backward()\n",
    "\n",
    "        return self.ls\n",
    "\n",
    "    def stopping(self):\n",
    "        # Stop the training if the median loss of the last min_ls_window steps is not improved in min_ls_wait by a factor of min_ls_tol\n",
    "        if self.iter > self.min_ls_wait + self.min_ls_window:\n",
    "\n",
    "            old_list = sorted(self.ls_hist[self.iter-self.min_ls_wait-self.min_ls_window+1:self.iter-self.min_ls_wait,0])\n",
    "            new_list = sorted(self.ls_hist[self.iter-self.min_ls_window+1:self.iter,0])\n",
    "            median_ind = self.min_ls_window//2\n",
    "\n",
    "            if new_list[median_ind] > old_list[median_ind] * (1-self.min_ls_tol):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def train(self):\n",
    "        self.net.train()\n",
    "\n",
    "        for i in range(1,self.pars['epochs']):\n",
    "            self.iter += 1\n",
    "\n",
    "            if self.pars['shuffle'] and i%self.pars['shuffle']==0:\n",
    "                self.sample_points()\n",
    "\n",
    "            try:\n",
    "                self.optimizer.step(self.closure)\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Stopped by user\")\n",
    "                self.save(0)\n",
    "                try:\n",
    "                    input('Press Enter to resume or Ctrl+C again to stop')\n",
    "                except KeyboardInterrupt:\n",
    "                    break\n",
    "                \n",
    "            if i%50 == 0:\n",
    "                print(f'Epoch: {self.iter}, Loss: {self.ls:.3e}, Loss_F: {self.ls_f:.3e}')\n",
    "                \n",
    "            self.ls_hist[i,:] = torch.hstack((self.ls,self.ls_f)).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fb1a6-412e-44db-b16c-517947dd6985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Loss: 6.642e-01, Loss_F: 6.642e-01\n",
      "Epoch: 100, Loss: 4.933e-01, Loss_F: 4.933e-01\n",
      "Epoch: 150, Loss: 3.747e-01, Loss_F: 3.747e-01\n",
      "Epoch: 200, Loss: 3.818e-01, Loss_F: 3.818e-01\n",
      "Epoch: 250, Loss: 2.799e-01, Loss_F: 2.799e-01\n",
      "Epoch: 300, Loss: 2.788e-01, Loss_F: 2.788e-01\n",
      "Epoch: 350, Loss: 2.916e-01, Loss_F: 2.916e-01\n",
      "Epoch: 400, Loss: 2.119e-01, Loss_F: 2.119e-01\n",
      "Epoch: 450, Loss: 2.007e-01, Loss_F: 2.007e-01\n",
      "Epoch: 500, Loss: 2.136e-01, Loss_F: 2.136e-01\n",
      "Epoch: 550, Loss: 2.512e-01, Loss_F: 2.512e-01\n",
      "Epoch: 600, Loss: 1.751e-01, Loss_F: 1.751e-01\n",
      "Epoch: 650, Loss: 2.316e-01, Loss_F: 2.316e-01\n",
      "Epoch: 700, Loss: 1.731e-01, Loss_F: 1.731e-01\n",
      "Epoch: 750, Loss: 1.683e-01, Loss_F: 1.683e-01\n",
      "Epoch: 800, Loss: 1.842e-01, Loss_F: 1.842e-01\n",
      "Epoch: 850, Loss: 1.466e-01, Loss_F: 1.466e-01\n",
      "Epoch: 900, Loss: 1.323e-01, Loss_F: 1.323e-01\n",
      "Epoch: 950, Loss: 2.021e-01, Loss_F: 2.021e-01\n",
      "Epoch: 1000, Loss: 1.547e-01, Loss_F: 1.547e-01\n",
      "Epoch: 1050, Loss: 1.228e-01, Loss_F: 1.228e-01\n",
      "Epoch: 1100, Loss: 1.579e-01, Loss_F: 1.579e-01\n",
      "Epoch: 1150, Loss: 1.294e-01, Loss_F: 1.294e-01\n",
      "Epoch: 1200, Loss: 1.215e-01, Loss_F: 1.215e-01\n",
      "Epoch: 1250, Loss: 1.062e-01, Loss_F: 1.062e-01\n",
      "Epoch: 1300, Loss: 1.274e-01, Loss_F: 1.274e-01\n",
      "Epoch: 1350, Loss: 1.268e-01, Loss_F: 1.268e-01\n",
      "Epoch: 1400, Loss: 9.090e-02, Loss_F: 9.090e-02\n",
      "Epoch: 1450, Loss: 8.570e-02, Loss_F: 8.570e-02\n",
      "Epoch: 1500, Loss: 1.492e-01, Loss_F: 1.492e-01\n",
      "Epoch: 1550, Loss: 7.708e-02, Loss_F: 7.708e-02\n",
      "Epoch: 1600, Loss: 1.238e-01, Loss_F: 1.238e-01\n",
      "Epoch: 1650, Loss: 8.083e-02, Loss_F: 8.083e-02\n",
      "Epoch: 1700, Loss: 8.458e-02, Loss_F: 8.458e-02\n",
      "Epoch: 1750, Loss: 1.267e-01, Loss_F: 1.267e-01\n",
      "Epoch: 1800, Loss: 1.064e-01, Loss_F: 1.064e-01\n",
      "Epoch: 1850, Loss: 7.848e-02, Loss_F: 7.848e-02\n",
      "Epoch: 1900, Loss: 8.087e-02, Loss_F: 8.087e-02\n",
      "Epoch: 1950, Loss: 8.423e-02, Loss_F: 8.423e-02\n",
      "Epoch: 2000, Loss: 1.054e-01, Loss_F: 1.054e-01\n",
      "Epoch: 2050, Loss: 1.023e-01, Loss_F: 1.023e-01\n",
      "Epoch: 2100, Loss: 7.423e-02, Loss_F: 7.423e-02\n",
      "Epoch: 2150, Loss: 6.622e-02, Loss_F: 6.622e-02\n",
      "Epoch: 2200, Loss: 9.120e-02, Loss_F: 9.120e-02\n",
      "Epoch: 2250, Loss: 8.716e-02, Loss_F: 8.716e-02\n",
      "Epoch: 2300, Loss: 8.473e-02, Loss_F: 8.473e-02\n",
      "Epoch: 2350, Loss: 9.762e-02, Loss_F: 9.762e-02\n",
      "Epoch: 2400, Loss: 6.216e-02, Loss_F: 6.216e-02\n",
      "Epoch: 2450, Loss: 9.200e-02, Loss_F: 9.200e-02\n",
      "Epoch: 2500, Loss: 1.061e-01, Loss_F: 1.061e-01\n",
      "Epoch: 2550, Loss: 7.097e-02, Loss_F: 7.097e-02\n",
      "Epoch: 2600, Loss: 7.759e-02, Loss_F: 7.759e-02\n",
      "Epoch: 2650, Loss: 8.777e-02, Loss_F: 8.777e-02\n",
      "Epoch: 2700, Loss: 5.386e-02, Loss_F: 5.386e-02\n",
      "Epoch: 2750, Loss: 8.912e-02, Loss_F: 8.912e-02\n",
      "Epoch: 2800, Loss: 7.837e-02, Loss_F: 7.837e-02\n"
     ]
    }
   ],
   "source": [
    "pars = {}\n",
    "pars['xi'] = -1\n",
    "pars['xf'] = 1\n",
    "pars['yi'] = -1\n",
    "pars['yf'] = 1\n",
    "pars['tf'] = 1\n",
    "pars['nu'] = 0.01/np.pi\n",
    "pars['layers'] = [20, 20, 20, 20]\n",
    "pars['loss_type'] = 'l1'\n",
    "pars['opt_method'] = 'adam'\n",
    "pars['epochs'] = 5000\n",
    "pars['opt_lr'] = 0.01\n",
    "pars['shuffle'] = 0\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = PINN(10000, pars, device=device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a853f-3ba3-4eaf-abfa-cf8ea387fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sympy import *\n",
    "import sympy as sp\n",
    "#import scipy.integrate as integrate\n",
    "import scipy.special as special\n",
    "from scipy.integrate import quad\n",
    "\n",
    "mu = 0.05\n",
    "dim = 2\n",
    "\n",
    "def u0(x):\n",
    "    return sin(np.pi*x)\n",
    "\n",
    "# x = sp.Symbol(\"x\").float()\n",
    "# x=x.float()\n",
    "\n",
    "def v0(x):\n",
    "    z = sp.Symbol(\"z\")\n",
    "    return integrate(u0(z), (z, 0, x))\n",
    "\n",
    "def w(x,mu):\n",
    "    return exp(-v0(x)/2/mu)\n",
    "\n",
    "c=[quad(w,0,1,args=(mu))[0]]\n",
    "\n",
    "def w0(x,k,mu):\n",
    "    return exp(-v0(x)/2/mu)*cos(k*np.pi*x)\n",
    "\n",
    "nb_sums=14;    \n",
    "\n",
    "for k in range(1,nb_sums):\n",
    "    c.append(2*quad(w0,0,1,args=(k,mu))[0])\n",
    "    \n",
    "def exact_example(t, x, T):\n",
    "    xi = (1+sum(x))/2\n",
    "    if t==T: \n",
    "        return np.sin(np.pi*xi)\n",
    "    else:\n",
    "        sum1 = 0  \n",
    "        for n in range(1,nb_sums):\n",
    "            sum1 += c[n]*n*np.exp(-n**2*np.pi**2*mu*dim*(T-t))*np.sin(n*np.pi*xi)\n",
    "        sum2 = 0\n",
    "        for n in range(1,nb_sums):\n",
    "            sum2 += c[n]*np.exp(-n**2*np.pi**2*mu*dim*(T-t))*np.cos(n*np.pi*xi)\n",
    "        return 2*np.pi*mu*sum1/(c[0]+sum2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec2c7c-c56b-4d04-b2d8-186f99f90e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "grid = np.expand_dims(np.linspace(-1, 1, 100), axis=0)\n",
    "tmax = 1\n",
    "time_list = [0, 0.1]\n",
    "\n",
    "for time in time_list:\n",
    "    t = time * np.ones((1, 100))\n",
    "    x = grid\n",
    "    y = 0 * np.ones((1, 100))\n",
    "    \n",
    "    grid_d_dim_with_t = np.concatenate([t.T, x.T, y.T], axis=1).astype('float32')\n",
    "    \n",
    "    pinn_sol = model.net(torch.from_numpy(grid_d_dim_with_t).to(device))\n",
    "    \n",
    "    initial = model.net.particular_solution(torch.from_numpy(grid_d_dim_with_t))\n",
    "    \n",
    "    plt.plot(grid[0], pinn_sol.cpu().detach().numpy()[:,0], label=f\"t={time:.2f}\")\n",
    "plt.title('y = 0')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(t,x,y)')\n",
    "plt.plot(grid[0], initial[:, 0], 'r--', label='Initial Condition')\n",
    "plt.plot(grid[0], exact_example(0, grid, 0.1), 'g--', label='True Solution')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61d92c-e120-4182-b0d1-31f3815de310",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.net.state_dict(), '../logs/models/pinn_test_case_1_dim_2_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
